{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "torch.manual_seed(99)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Initial data importing and spliting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_full_df = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "# Splitting the full dataset into training, validation and testing data with a 70:15:15 ratio\n",
    "diabetes_train_df, diabetes_val_test_df = train_test_split(\n",
    "    diabetes_full_df, train_size = 0.7, stratify = diabetes_full_df.Outcome, random_state = 93)\n",
    "\n",
    "diabetes_val_df, diabetes_test_df = train_test_split(\n",
    "    diabetes_val_test_df, train_size = 0.5, stratify = diabetes_val_test_df.Outcome, random_state = 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the predictor variables\n",
    "diabetes_features = [var for var in diabetes_train_df.columns if var != 'Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scaling the data to have mean of zero and standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler().set_output(transform = \"pandas\")\n",
    "\n",
    "# Extracting the predictors\n",
    "diabetes_features = [var for var in diabetes_train_df.columns if var != 'Outcome']\n",
    "\n",
    "# Apply the Standard Scaler to the X (faetures) of the training data (not including the target variable 'Outcome')\n",
    "diabetes_train_preprocessed_df = std_scaler.fit_transform(diabetes_train_df[diabetes_features])\n",
    "\n",
    "# Put back the target variable 'Outcome' \n",
    "diabetes_train_preprocessed_df['Outcome'] = diabetes_train_df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronise the preprocessing step of scaling onto the validation data\n",
    "diabetes_val_processed_df = std_scaler.transform(diabetes_val_df[diabetes_features])\n",
    "diabetes_val_processed_df['Outcome'] = diabetes_val_df['Outcome'].astype('category')\n",
    "\n",
    "# Synchronise the preprocessing step of scaling onto the testing data\n",
    "diabetes_test_processed_df = std_scaler.transform(diabetes_test_df[diabetes_features])\n",
    "diabetes_test_processed_df['Outcome'] = diabetes_test_df['Outcome'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Oversampling the training data using SMOTE \n",
    "\n",
    "*(This preprocessing step is just to facilitate the training process, so it will not get inferred onto the validation and testing data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018881</td>\n",
       "      <td>-0.636730</td>\n",
       "      <td>-3.385990</td>\n",
       "      <td>-1.266133</td>\n",
       "      <td>-0.669042</td>\n",
       "      <td>-0.228058</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>-0.072585</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.256052</td>\n",
       "      <td>-0.541433</td>\n",
       "      <td>-0.021468</td>\n",
       "      <td>1.286814</td>\n",
       "      <td>-0.669042</td>\n",
       "      <td>1.793098</td>\n",
       "      <td>-1.037973</td>\n",
       "      <td>0.789765</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.836876</td>\n",
       "      <td>0.189173</td>\n",
       "      <td>-0.615207</td>\n",
       "      <td>0.584753</td>\n",
       "      <td>0.612248</td>\n",
       "      <td>-0.390249</td>\n",
       "      <td>1.001497</td>\n",
       "      <td>-1.021171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.527583</td>\n",
       "      <td>-0.414371</td>\n",
       "      <td>0.275401</td>\n",
       "      <td>0.648577</td>\n",
       "      <td>0.173912</td>\n",
       "      <td>0.221088</td>\n",
       "      <td>-0.198014</td>\n",
       "      <td>-0.848701</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.836876</td>\n",
       "      <td>-0.033185</td>\n",
       "      <td>0.968097</td>\n",
       "      <td>1.350637</td>\n",
       "      <td>0.763980</td>\n",
       "      <td>1.680811</td>\n",
       "      <td>0.113194</td>\n",
       "      <td>-0.589995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>-0.501630</td>\n",
       "      <td>1.764122</td>\n",
       "      <td>-0.004861</td>\n",
       "      <td>0.752346</td>\n",
       "      <td>0.642696</td>\n",
       "      <td>0.229377</td>\n",
       "      <td>-0.638878</td>\n",
       "      <td>-0.209473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>2.171464</td>\n",
       "      <td>0.724015</td>\n",
       "      <td>0.553677</td>\n",
       "      <td>0.758235</td>\n",
       "      <td>0.431409</td>\n",
       "      <td>-0.535693</td>\n",
       "      <td>-0.578563</td>\n",
       "      <td>0.907686</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>4.068460</td>\n",
       "      <td>1.354957</td>\n",
       "      <td>0.188333</td>\n",
       "      <td>1.359583</td>\n",
       "      <td>0.312685</td>\n",
       "      <td>1.123612</td>\n",
       "      <td>1.060971</td>\n",
       "      <td>1.227847</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>1.051600</td>\n",
       "      <td>1.929450</td>\n",
       "      <td>1.093060</td>\n",
       "      <td>-1.266133</td>\n",
       "      <td>-0.669042</td>\n",
       "      <td>0.223603</td>\n",
       "      <td>-0.551986</td>\n",
       "      <td>2.319046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2.804140</td>\n",
       "      <td>-0.948777</td>\n",
       "      <td>0.291047</td>\n",
       "      <td>0.564572</td>\n",
       "      <td>-0.070289</td>\n",
       "      <td>0.066836</td>\n",
       "      <td>-0.387174</td>\n",
       "      <td>1.134706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0       1.018881 -0.636730      -3.385990      -1.266133 -0.669042 -0.228058   \n",
       "1       2.256052 -0.541433      -0.021468       1.286814 -0.669042  1.793098   \n",
       "2      -0.836876  0.189173      -0.615207       0.584753  0.612248 -0.390249   \n",
       "3      -0.527583 -0.414371       0.275401       0.648577  0.173912  0.221088   \n",
       "4      -0.836876 -0.033185       0.968097       1.350637  0.763980  1.680811   \n",
       "..           ...       ...            ...            ...       ...       ...   \n",
       "695    -0.501630  1.764122      -0.004861       0.752346  0.642696  0.229377   \n",
       "696     2.171464  0.724015       0.553677       0.758235  0.431409 -0.535693   \n",
       "697     4.068460  1.354957       0.188333       1.359583  0.312685  1.123612   \n",
       "698     1.051600  1.929450       1.093060      -1.266133 -0.669042  0.223603   \n",
       "699     2.804140 -0.948777       0.291047       0.564572 -0.070289  0.066836   \n",
       "\n",
       "     DiabetesPedigreeFunction       Age Outcome  \n",
       "0                    0.043701 -0.072585       1  \n",
       "1                   -1.037973  0.789765       0  \n",
       "2                    1.001497 -1.021171       0  \n",
       "3                   -0.198014 -0.848701       0  \n",
       "4                    0.113194 -0.589995       0  \n",
       "..                        ...       ...     ...  \n",
       "695                 -0.638878 -0.209473       1  \n",
       "696                 -0.578563  0.907686       1  \n",
       "697                  1.060971  1.227847       1  \n",
       "698                 -0.551986  2.319046       1  \n",
       "699                 -0.387174  1.134706       1  \n",
       "\n",
       "[700 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote_spec = SMOTE(random_state = 95, k_neighbors = 5)\n",
    "\n",
    "# Apply the SMOTE on the training data\n",
    "diabetes_X_train_oversampled, diabetes_y_train_oversampled = smote_spec.fit_resample(\n",
    "    X = diabetes_train_preprocessed_df[diabetes_features], y = diabetes_train_preprocessed_df['Outcome'])\n",
    "\n",
    "# The SMOTE-oversampling output is a NumPy array, so update the preprocessed training data in a Pandas dataframe\n",
    "diabetes_train_preprocessed_df = pd.DataFrame(diabetes_X_train_oversampled, columns = diabetes_features)\n",
    "diabetes_train_preprocessed_df['Outcome'] = diabetes_y_train_oversampled.astype('category')\n",
    "diabetes_train_preprocessed_df # It can be seen now that the training data has 700 instances "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Loading the datasets using Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetObject(Dataset):\n",
    "    def __init__(self, dataframe, target_var):\n",
    "        self.data = dataframe\n",
    "        self.target = target_var\n",
    "        self.predictors = [var for var in self.data.columns if var != target_var]\n",
    "        self.X = self.data[self.predictors]\n",
    "        self.y = self.data[target_var]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, torch.Tensor):\n",
    "            index = index.tolist()\n",
    "\n",
    "        return [self.X.iloc[index, :].values, self.y.values[index]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_train_dataset = DatasetObject(diabetes_train_preprocessed_df, target_var = 'Outcome')\n",
    "diabetes_val_dataset = DatasetObject(diabetes_val_processed_df, target_var = 'Outcome')\n",
    "diabetes_test_dataset = DatasetObject(diabetes_test_processed_df, target_var = 'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_train = DataLoader(diabetes_train_dataset, batch_size = 32, shuffle = False)\n",
    "diabetes_val = DataLoader(diabetes_val_dataset, batch_size = 32, shuffle = False)\n",
    "diabetes_test = DataLoader(diabetes_test_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparing functions to train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, criterion, optim, epochs, train_input, val_input, print_log: bool):\n",
    "\n",
    "    training_log = pd.DataFrame({'epoch': [], 'training_loss' : [], 'training_accuracy': [],\n",
    "                                 'validation_loss': [], 'validation_accuracy': []})\n",
    "    for epoch in range(epochs):\n",
    "        # Training the model on the training set\n",
    "        total_train_loss = 0\n",
    "        y_train_true, y_train_pred = [], []\n",
    "        for X_train, y_train in train_input:\n",
    "            model.train() # Set the model to training mode\n",
    "            optim.zero_grad()\n",
    "            X_train = X_train.to(torch.float32)\n",
    "            outputs = model(X_train)\n",
    "            _, predicted_class = torch.max(outputs, 1)\n",
    "\n",
    "            y_train_true.extend(np.array(y_train))\n",
    "            y_train_pred.extend(np.array(predicted_class))\n",
    "            current_batch_train_loss = criterion(outputs, y_train) # The training_loss at the last epoch will be recorded\n",
    "            total_train_loss += current_batch_train_loss.item()\n",
    "            current_batch_train_loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        train_loss = total_train_loss / len(train_input) # Total accumulated loss divided by the number of batches\n",
    "        train_accuracy = accuracy_score(y_true = y_train_true, y_pred = y_train_pred)\n",
    "\n",
    "        # Evaluating on the validation set\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        y_val_true, y_val_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_input:\n",
    "                X_val = X_val.to(torch.float32)\n",
    "                outputs = model(X_val)\n",
    "                current_batch_val_loss = criterion(outputs, y_val) # The val_loss at the last epoch will be recorded\n",
    "                total_val_loss += current_batch_val_loss.item()\n",
    "                _, predicted_class = torch.max(outputs, 1)\n",
    "                y_val_true.extend(np.array(y_val))\n",
    "                y_val_pred.extend(np.array(predicted_class))\n",
    "        \n",
    "        val_loss = total_val_loss / len(val_input) # Total accumulated loss divided by the number of batches\n",
    "        val_accuracy = accuracy_score(y_true = y_val_true, y_pred = y_val_pred)\n",
    "        \n",
    "        if print_log == True:\n",
    "            print('Epoch:', epoch+1, '| Training Loss:', round(train_loss, 3), '| Training Accuracy:', round(train_accuracy, 3), \n",
    "                  '| Validation Loss:', round(val_loss, 3), '| Validation Accuracy:', round(val_accuracy, 3), sep = \" \")\n",
    "            \n",
    "        training_log.loc[len(training_log)] = [epoch, train_loss, train_accuracy, val_loss, val_accuracy]\n",
    "    \n",
    "    return training_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Experimenting with the Single-layer perceptron (SLP) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySLP(nn.Module):\n",
    "    def __init__(self, input_size, no_of_output_classes):\n",
    "        super(MySLP, self).__init__()\n",
    "        self.layer_out = nn.Linear(input_size, no_of_output_classes)\n",
    "\n",
    "    def forward(self, X_input):\n",
    "        output = self.layer_out(X_input)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Optimising some hyperparameters for the SLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_hyperparams_space = {'epochs': [50, 75, 100, 125, 150],\n",
    "                         'lr': [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
    "                         'optimiser': ['SGD', 'Adam', 'RMSprop']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_hyperparams_tune_result = pd.DataFrame(\n",
    "    {'max_epochs': [], 'learning_rate': [], 'optimiser': [], 'validation_accuracy': []})\n",
    "\n",
    "for epochs in slp_hyperparams_space['epochs']:\n",
    "    for lr in slp_hyperparams_space['lr']:\n",
    "        for optimiser in slp_hyperparams_space['optimiser']:\n",
    "            torch.manual_seed(99) # For reproducibility purpose\n",
    "            my_model_1 = MySLP(8, 2)\n",
    "            my_criterion_1 = nn.CrossEntropyLoss()\n",
    "\n",
    "            if optimiser == 'SGD':\n",
    "                my_optimizer_1 = optim.SGD(params = my_model_1.parameters(), lr = lr)\n",
    "            if optimiser == 'Adam':\n",
    "                my_optimizer_1 = optim.Adam(params = my_model_1.parameters(), lr = lr)\n",
    "            if optimiser == 'RMSprop':\n",
    "                my_optimizer_1 = optim.RMSprop(params = my_model_1.parameters(), lr = lr)\n",
    "\n",
    "            training_log_df = train_and_validate_model(\n",
    "                model = my_model_1, criterion = my_criterion_1, optim = my_optimizer_1, \n",
    "                epochs = epochs, train_input = diabetes_train, val_input = diabetes_val, print_log = False)\n",
    "            \n",
    "            val_accuracy = training_log_df['validation_accuracy'].values[-1]\n",
    "\n",
    "            slp_hyperparams_tune_result.loc[len(slp_hyperparams_tune_result)] = [\n",
    "                epochs, lr, optimiser, val_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the hyperparameter tuning results by the descending order of validation accuracy and the ascending of max_epochs\n",
    "slp_hyperparams_tune_result.sort_values([\"validation_accuracy\", \"max_epochs\"], ascending = [False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Train a Single-Layer Perceptron model with the best hyperparameter values chosen from above and evaluate it on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best SLP model again for a closer inspection\n",
    "torch.manual_seed(99) # For reproducibility purpose\n",
    "my_slp_model = MySLP(8, 2)\n",
    "my_criterion_1 = nn.CrossEntropyLoss()\n",
    "my_optimizer_1 = optim.RMSprop(my_slp_model.parameters(), lr = 0.001)\n",
    "\n",
    "my_slp_model_training_log = train_and_validate_model(\n",
    "    model = my_slp_model, criterion = my_criterion_1, optim = my_optimizer_1, \n",
    "    epochs = 50, train_input = diabetes_train, val_input = diabetes_val, print_log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the learning curves of the best SLP model\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(my_slp_model_training_log['epoch'], my_slp_model_training_log['training_accuracy'], label = \"training_accuracy\")\n",
    "plt.plot(my_slp_model_training_log['epoch'], my_slp_model_training_log['validation_accuracy'], label = \"validation accuracy\")\n",
    "plt.plot(my_slp_model_training_log['epoch'], my_slp_model_training_log['training_loss'], label = \"training_loss\")\n",
    "plt.plot(my_slp_model_training_log['epoch'], my_slp_model_training_log['validation_loss'], label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss | Accuracy')\n",
    "plt.title('The best SLP model setting {epochs = 50, learning rate = 0.001, optimiser = RMSprop}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multi-layer perceptron (MLP) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes: list, no_of_output_classes):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for hidden_size, corresponding_act_fn in hidden_sizes:\n",
    "            self.hidden.append(nn.Linear(self.input_size, hidden_size))\n",
    "            if corresponding_act_fn is not None:\n",
    "                self.hidden.append(corresponding_act_fn) # Assume that 'corresponding_act_fn' is input correctly (under the 'nn' class)\n",
    "            self.input_size = hidden_size\n",
    "        \n",
    "        self.layer_out = nn.Linear(self.input_size, no_of_output_classes)\n",
    "\n",
    "    def forward(self, X_input):\n",
    "        for layer in self.hidden:\n",
    "            X_input = layer(X_input)\n",
    "        output = self.layer_out(X_input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Optimising some hyperparameters for the MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is desired that each hidden layer could have its own activation function instead of using a fixed one, the following list of 'hidden_sizes_space' will be created before optimising the MLP hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Creating a custom list of all possible combinations between the hidden sizes and the activation functions chosen\n",
    "hidden_sizes_space = [16, 32, 64]\n",
    "act_fn_space = [None, nn.ReLU(), nn.Tanh()]\n",
    "\n",
    "hidden_sizes_with_act_fn = []\n",
    "\n",
    "one_layer_list = list(itertools.combinations(list(itertools.product(hidden_sizes_space, act_fn_space)), r = 1))\n",
    "for layer in one_layer_list:\n",
    "    hidden_sizes_with_act_fn.append([layer[0]])\n",
    "\n",
    "two_layers_list = list(itertools.combinations(list(itertools.product(hidden_sizes_space, act_fn_space)), r = 2))\n",
    "for layer_1, layer_2 in two_layers_list: # Excluded if a layer has more nodes than its previous layer(s)\n",
    "    if layer_1[0] < layer_2[0]:\n",
    "        hidden_sizes_with_act_fn.append([layer_1, layer_2])\n",
    "\n",
    "three_layers_list = list(itertools.combinations(list(itertools.product(hidden_sizes_space, act_fn_space)), r = 3))\n",
    "for layer_1, layer_2, layer_3 in three_layers_list: # Excluded if a layer has more nodes than its previous layer(s)\n",
    "    if layer_1[0] < layer_2[0] and layer_2[0] < layer_3[0]:\n",
    "        hidden_sizes_with_act_fn.append([layer_1, layer_2, layer_3])\n",
    "\n",
    "hidden_sizes_with_act_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_hyperparams_space = {'epochs': [50, 75, 100, 125, 150],\n",
    "                     'hidden_sizes': hidden_sizes_with_act_fn,\n",
    "                     'lr': [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
    "                     'optimiser': ['SGD', 'Adam', 'RMSprop']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_hyperparams_tune_result = pd.DataFrame(\n",
    "    {'max_epochs': [], 'hidden_layer_sizes_with_activation_functions': [],\n",
    "     'learning_rate': [], 'optimiser': [], 'validation_accuracy': []})\n",
    "\n",
    "for epochs in mlp_hyperparams_space['epochs']:\n",
    "    for hidden_sizes in mlp_hyperparams_space['hidden_sizes']:\n",
    "        for lr in mlp_hyperparams_space['lr']:\n",
    "            for optimiser in mlp_hyperparams_space['optimiser']:\n",
    "                torch.manual_seed(99) # For reproducibility purpose\n",
    "                my_model_2 = MyMLP(8, hidden_sizes, 2)\n",
    "                my_criterion_2 = nn.CrossEntropyLoss()\n",
    "\n",
    "                if optimiser == 'SGD':\n",
    "                    my_optimizer_2 = optim.SGD(params = my_model_2.parameters(), lr = lr)\n",
    "                if optimiser == 'Adam':\n",
    "                    my_optimizer_2 = optim.Adam(params = my_model_2.parameters(), lr = lr)\n",
    "                if optimiser == 'RMSprop':\n",
    "                    my_optimizer_2 = optim.RMSprop(params = my_model_2.parameters(), lr = lr)\n",
    "\n",
    "                training_log_df = train_and_validate_model(\n",
    "                    model = my_model_2, criterion = my_criterion_2, optim = my_optimizer_2, \n",
    "                    epochs = epochs, train_input = diabetes_train, val_input = diabetes_val, print_log = False)\n",
    "                \n",
    "                val_accuracy = training_log_df['validation_accuracy'].values[-1]\n",
    "\n",
    "                mlp_hyperparams_tune_result.loc[len(mlp_hyperparams_tune_result)] = [\n",
    "                    epochs, hidden_sizes, lr, optimiser, val_accuracy]\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the hyperparameter tuning results by the reversed order of validation accuracy\n",
    "mlp_hyperparams_tune_result.sort_values(['validation_accuracy'], ascending = [False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Train a Multi-Layer Perceptron model with the best hyperparameter values chosen from above and evaluate it on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best MLP model again for a closer inspection\n",
    "torch.manual_seed(99) # For reproducibility purpose\n",
    "my_mlp_model = MyMLP(8, [(16, None), (32, None), (64, nn.Tanh())], 2)\n",
    "my_criterion_2 = nn.CrossEntropyLoss()\n",
    "my_optimizer_2 = optim.Adam(my_mlp_model.parameters(), lr = 0.001)\n",
    "\n",
    "my_mlp_model_training_log = train_and_validate_model(\n",
    "    model = my_mlp_model, criterion = my_criterion_2, optim = my_optimizer_2, \n",
    "    epochs = 125, train_input = diabetes_train, val_input = diabetes_val, print_log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the learning curve of the best MLP model\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(my_mlp_model_training_log['epoch'], my_mlp_model_training_log['training_accuracy'], label = \"training_accuracy\")\n",
    "plt.plot(my_mlp_model_training_log['epoch'], my_mlp_model_training_log['validation_accuracy'], label = \"validation accuracy\")\n",
    "plt.plot(my_mlp_model_training_log['epoch'], my_mlp_model_training_log['training_loss'], label = \"training_loss\")\n",
    "plt.plot(my_mlp_model_training_log['epoch'], my_mlp_model_training_log['validation_loss'], label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss | Accuracy')\n",
    "plt.title('The best MLP model setting\\n{hidden_layers = [(16, None), (32, None), (64, Tanh)],\\nepochs = 125, learning rate = 0.001, optimiser = Adam}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Final evaluation of the chosen model setting on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, test_input):\n",
    "    \"\"\"This function is just to condense some common metrics for the final \n",
    "    evaluation stage on the testing data after an optimal model has been chosen.\"\"\"\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_input:\n",
    "            X = X.to(torch.float32)\n",
    "            outputs = model(X)\n",
    "            _, predicted_class = torch.max(outputs, 1)\n",
    "            y_true.extend(np.array(y))\n",
    "            y_pred.extend(np.array(predicted_class))\n",
    "\n",
    "    condensed_evaluation = {}\n",
    "    condensed_evaluation['recall'] = recall_score(y_true = y_true, y_pred = y_pred, pos_label = 1)\n",
    "    condensed_evaluation['precision'] = precision_score(y_true = y_true, y_pred = y_pred, pos_label = 1)\n",
    "    condensed_evaluation['f1_score'] = f1_score(y_true = y_true, y_pred = y_pred, pos_label = 1)\n",
    "    condensed_evaluation['balanced_accuracy'] = balanced_accuracy_score(y_true = y_true, y_pred = y_pred)\n",
    "    condensed_evaluation['confusion_matrix'] = confusion_matrix(y_true = y_true, y_pred = y_pred)\n",
    "\n",
    "    return condensed_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Displaying a confusion matrix on the testing data\n",
    "diabetes_test_cf = evaluate_model(my_mlp_model, diabetes_test)['confusion_matrix']\n",
    "diabetes_test_cf_disp = ConfusionMatrixDisplay(diabetes_test_cf)\n",
    "diabetes_test_cf_disp.plot(cmap = plt.cm.Blues)\n",
    "for labels in diabetes_test_cf_disp.text_.ravel():\n",
    "    labels.set_fontsize(30)\n",
    "plt.xlabel(xlabel = \"Predicted label\", fontdict = {'size': 18})\n",
    "plt.ylabel(ylabel = \"True label\", fontdict = {'size': 18})\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the test-evaluation results\n",
    "evaluate_model(my_mlp_model, diabetes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
